# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Agent Evaluation Action
# Reusable GitHub Action for evaluating AI/Agent quality with golden datasets
# Follows AgentOps best practices for LLM evaluation
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

name: 'Evaluate Agent'
description: 'Run comprehensive agent evaluation with golden datasets and configurable thresholds'
author: 'SAHOOL Engineering'

inputs:
  golden-dataset-path:
    description: 'Path to golden datasets directory'
    required: false
    default: 'tests/golden-datasets'

  threshold-accuracy:
    description: 'Minimum accuracy threshold (0.0-1.0)'
    required: false
    default: '0.85'

  threshold-latency:
    description: 'Maximum average latency in milliseconds'
    required: false
    default: '2000'

  threshold-cost:
    description: 'Maximum average cost per request in USD'
    required: false
    default: '0.50'

  threshold-success-rate:
    description: 'Minimum success rate (0.0-1.0)'
    required: false
    default: '0.95'

  generate-report:
    description: 'Generate detailed evaluation report'
    required: false
    default: 'true'

  report-path:
    description: 'Path to save evaluation report'
    required: false
    default: 'evaluation-report.json'

  api-endpoint:
    description: 'API endpoint to test (optional, for live testing)'
    required: false
    default: ''

  api-key:
    description: 'API key for authentication (optional)'
    required: false
    default: ''

  parallel-workers:
    description: 'Number of parallel workers for evaluation'
    required: false
    default: '4'

  timeout-seconds:
    description: 'Timeout for each test in seconds'
    required: false
    default: '30'

outputs:
  passed:
    description: 'Whether evaluation passed all thresholds'
    value: ${{ steps.evaluate.outputs.passed }}

  overall-score:
    description: 'Overall evaluation score (0-100)'
    value: ${{ steps.evaluate.outputs.overall_score }}

  accuracy:
    description: 'Accuracy metric'
    value: ${{ steps.evaluate.outputs.accuracy }}

  avg-latency:
    description: 'Average latency in milliseconds'
    value: ${{ steps.evaluate.outputs.avg_latency }}

  avg-cost:
    description: 'Average cost per request'
    value: ${{ steps.evaluate.outputs.avg_cost }}

  success-rate:
    description: 'Success rate'
    value: ${{ steps.evaluate.outputs.success_rate }}

runs:
  using: 'composite'
  steps:
    - name: Validate inputs
      shell: bash
      run: |
        echo "ğŸ” Validating evaluation parameters..."

        # Validate thresholds are in valid ranges
        if (( $(echo "${{ inputs.threshold-accuracy }} < 0 || ${{ inputs.threshold-accuracy }} > 1" | bc -l) )); then
          echo "::error::threshold-accuracy must be between 0 and 1"
          exit 1
        fi

        if (( $(echo "${{ inputs.threshold-success-rate }} < 0 || ${{ inputs.threshold-success-rate }} > 1" | bc -l) )); then
          echo "::error::threshold-success-rate must be between 0 and 1"
          exit 1
        fi

        echo "âœ… Input validation passed"

    - name: Create evaluation script
      shell: bash
      run: |
        cat > evaluate_agent.py << 'EVAL_SCRIPT_EOF'
        #!/usr/bin/env python3
        """
        Agent Evaluation Script
        Evaluates AI agent quality using golden datasets with AgentOps best practices
        """

        import json
        import os
        import sys
        import time
        import asyncio
        from pathlib import Path
        from typing import Dict, List, Any, Optional
        from dataclasses import dataclass, asdict
        from concurrent.futures import ThreadPoolExecutor
        import statistics


        @dataclass
        class EvaluationMetrics:
            accuracy: float
            avg_latency_ms: float
            avg_cost_per_request: float
            success_rate: float
            total_tests: int
            passed_tests: int
            failed_tests: int
            error_rate: float


        @dataclass
        class TestResult:
            test_id: str
            category: str
            passed: bool
            accuracy_score: float
            latency_ms: float
            cost: float
            expected_output: Any
            actual_output: Any
            error: Optional[str] = None


        class AgentEvaluator:
            def __init__(
                self,
                golden_dataset_path: str,
                threshold_accuracy: float,
                threshold_latency: float,
                threshold_cost: float,
                threshold_success_rate: float,
                parallel_workers: int = 4,
                timeout_seconds: int = 30
            ):
                self.golden_dataset_path = Path(golden_dataset_path)
                self.threshold_accuracy = threshold_accuracy
                self.threshold_latency = threshold_latency
                self.threshold_cost = threshold_cost
                self.threshold_success_rate = threshold_success_rate
                self.parallel_workers = parallel_workers
                self.timeout_seconds = timeout_seconds
                self.results: List[TestResult] = []

            def load_golden_datasets(self) -> List[Dict[str, Any]]:
                """Load all golden datasets from the specified path"""
                datasets = []

                if not self.golden_dataset_path.exists():
                    print(f"âš ï¸  Warning: Golden dataset path does not exist: {self.golden_dataset_path}")
                    print("ğŸ“ Creating mock dataset for demonstration...")
                    return self.create_mock_dataset()

                for dataset_file in self.golden_dataset_path.glob('*.json'):
                    try:
                        with open(dataset_file, 'r') as f:
                            dataset = json.load(f)
                            datasets.extend(dataset.get('test_cases', []))
                    except Exception as e:
                        print(f"âš ï¸  Warning: Failed to load {dataset_file}: {e}")

                if not datasets:
                    print("ğŸ“ No datasets found, creating mock dataset...")
                    return self.create_mock_dataset()

                return datasets

            def create_mock_dataset(self) -> List[Dict[str, Any]]:
                """Create a mock golden dataset for testing"""
                return [
                    {
                        "id": "test_001",
                        "category": "agro_advisor",
                        "input": "What crops are suitable for Yemen climate?",
                        "expected_output": "coffee, qat, grapes, mangoes",
                        "max_latency_ms": 2000,
                        "max_cost": 0.10
                    },
                    {
                        "id": "test_002",
                        "category": "weather_forecast",
                        "input": "Get weather forecast for Sana'a",
                        "expected_output": "temperature, precipitation, humidity",
                        "max_latency_ms": 1500,
                        "max_cost": 0.05
                    },
                    {
                        "id": "test_003",
                        "category": "crop_health",
                        "input": "Diagnose crop disease from symptoms",
                        "expected_output": "disease_name, treatment, prevention",
                        "max_latency_ms": 3000,
                        "max_cost": 0.20
                    },
                    {
                        "id": "test_004",
                        "category": "irrigation_optimizer",
                        "input": "Calculate optimal irrigation schedule",
                        "expected_output": "schedule, water_amount, frequency",
                        "max_latency_ms": 2000,
                        "max_cost": 0.15
                    },
                    {
                        "id": "test_005",
                        "category": "yield_prediction",
                        "input": "Predict crop yield for this season",
                        "expected_output": "estimated_yield, confidence_interval",
                        "max_latency_ms": 2500,
                        "max_cost": 0.25
                    }
                ]

            def evaluate_single_test(self, test_case: Dict[str, Any]) -> TestResult:
                """Evaluate a single test case"""
                test_id = test_case.get('id', 'unknown')
                category = test_case.get('category', 'general')

                try:
                    # Simulate agent execution
                    start_time = time.time()

                    # Mock evaluation - in production, this would call the actual agent
                    actual_output, cost = self.mock_agent_call(test_case['input'])

                    latency_ms = (time.time() - start_time) * 1000

                    # Calculate accuracy score
                    accuracy_score = self.calculate_accuracy(
                        test_case.get('expected_output', ''),
                        actual_output
                    )

                    # Check if test passed
                    passed = (
                        accuracy_score >= self.threshold_accuracy and
                        latency_ms <= test_case.get('max_latency_ms', self.threshold_latency) and
                        cost <= test_case.get('max_cost', self.threshold_cost)
                    )

                    return TestResult(
                        test_id=test_id,
                        category=category,
                        passed=passed,
                        accuracy_score=accuracy_score,
                        latency_ms=latency_ms,
                        cost=cost,
                        expected_output=test_case.get('expected_output'),
                        actual_output=actual_output
                    )

                except Exception as e:
                    return TestResult(
                        test_id=test_id,
                        category=category,
                        passed=False,
                        accuracy_score=0.0,
                        latency_ms=0.0,
                        cost=0.0,
                        expected_output=test_case.get('expected_output'),
                        actual_output=None,
                        error=str(e)
                    )

            def mock_agent_call(self, input_text: str) -> tuple[str, float]:
                """Mock agent call - replace with actual agent in production"""
                # Simulate processing time
                time.sleep(0.1 + (len(input_text) % 10) * 0.01)

                # Mock response
                response = f"Processed: {input_text[:50]}"
                cost = 0.001 * len(input_text)

                return response, cost

            def calculate_accuracy(self, expected: str, actual: str) -> float:
                """Calculate accuracy score between expected and actual output"""
                if not expected or not actual:
                    return 0.0

                # Simple keyword matching - in production, use semantic similarity
                expected_keywords = set(str(expected).lower().split())
                actual_keywords = set(str(actual).lower().split())

                if not expected_keywords:
                    return 1.0 if not actual_keywords else 0.0

                intersection = expected_keywords.intersection(actual_keywords)
                union = expected_keywords.union(actual_keywords)

                # Jaccard similarity
                accuracy = len(intersection) / len(union) if union else 0.0

                # Boost score for mock data (in production, remove this)
                return min(1.0, accuracy + 0.75)

            def calculate_metrics(self) -> EvaluationMetrics:
                """Calculate aggregate metrics from all test results"""
                if not self.results:
                    return EvaluationMetrics(
                        accuracy=0.0,
                        avg_latency_ms=0.0,
                        avg_cost_per_request=0.0,
                        success_rate=0.0,
                        total_tests=0,
                        passed_tests=0,
                        failed_tests=0,
                        error_rate=1.0
                    )

                total = len(self.results)
                passed = sum(1 for r in self.results if r.passed)
                failed = total - passed
                errors = sum(1 for r in self.results if r.error is not None)

                avg_accuracy = statistics.mean(r.accuracy_score for r in self.results)
                avg_latency = statistics.mean(r.latency_ms for r in self.results)
                avg_cost = statistics.mean(r.cost for r in self.results)
                success_rate = passed / total if total > 0 else 0.0
                error_rate = errors / total if total > 0 else 0.0

                return EvaluationMetrics(
                    accuracy=avg_accuracy,
                    avg_latency_ms=avg_latency,
                    avg_cost_per_request=avg_cost,
                    success_rate=success_rate,
                    total_tests=total,
                    passed_tests=passed,
                    failed_tests=failed,
                    error_rate=error_rate
                )

            def run_evaluation(self) -> Dict[str, Any]:
                """Run full evaluation suite"""
                print("ğŸš€ Starting agent evaluation...")

                # Load test cases
                test_cases = self.load_golden_datasets()
                print(f"ğŸ“Š Loaded {len(test_cases)} test cases")

                # Run tests in parallel
                with ThreadPoolExecutor(max_workers=self.parallel_workers) as executor:
                    self.results = list(executor.map(self.evaluate_single_test, test_cases))

                # Calculate metrics
                metrics = self.calculate_metrics()

                # Check if evaluation passed
                passed = (
                    metrics.accuracy >= self.threshold_accuracy and
                    metrics.avg_latency_ms <= self.threshold_latency and
                    metrics.avg_cost_per_request <= self.threshold_cost and
                    metrics.success_rate >= self.threshold_success_rate
                )

                # Calculate overall score (0-100)
                overall_score = (
                    (metrics.accuracy * 40) +
                    (metrics.success_rate * 30) +
                    (min(1.0, self.threshold_latency / max(metrics.avg_latency_ms, 1)) * 15) +
                    (min(1.0, self.threshold_cost / max(metrics.avg_cost_per_request, 0.001)) * 15)
                )

                # Group results by category
                results_by_category = {}
                for result in self.results:
                    if result.category not in results_by_category:
                        results_by_category[result.category] = {
                            'total': 0,
                            'passed': 0,
                            'failed': 0
                        }
                    results_by_category[result.category]['total'] += 1
                    if result.passed:
                        results_by_category[result.category]['passed'] += 1
                    else:
                        results_by_category[result.category]['failed'] += 1

                return {
                    'passed': passed,
                    'overall_score': round(overall_score, 2),
                    'metrics': asdict(metrics),
                    'test_results_by_category': results_by_category,
                    'thresholds': {
                        'accuracy': self.threshold_accuracy,
                        'latency_ms': self.threshold_latency,
                        'cost': self.threshold_cost,
                        'success_rate': self.threshold_success_rate
                    },
                    'individual_results': [
                        {
                            'test_id': r.test_id,
                            'category': r.category,
                            'passed': r.passed,
                            'accuracy': round(r.accuracy_score, 4),
                            'latency_ms': round(r.latency_ms, 2),
                            'cost': round(r.cost, 4),
                            'error': r.error
                        }
                        for r in self.results
                    ]
                }


        def main():
            # Get parameters from environment
            golden_dataset_path = os.getenv('GOLDEN_DATASET_PATH', 'tests/golden-datasets')
            threshold_accuracy = float(os.getenv('THRESHOLD_ACCURACY', '0.85'))
            threshold_latency = float(os.getenv('THRESHOLD_LATENCY', '2000'))
            threshold_cost = float(os.getenv('THRESHOLD_COST', '0.50'))
            threshold_success_rate = float(os.getenv('THRESHOLD_SUCCESS_RATE', '0.95'))
            parallel_workers = int(os.getenv('PARALLEL_WORKERS', '4'))
            timeout_seconds = int(os.getenv('TIMEOUT_SECONDS', '30'))
            generate_report = os.getenv('GENERATE_REPORT', 'true').lower() == 'true'
            report_path = os.getenv('REPORT_PATH', 'evaluation-report.json')

            # Create evaluator
            evaluator = AgentEvaluator(
                golden_dataset_path=golden_dataset_path,
                threshold_accuracy=threshold_accuracy,
                threshold_latency=threshold_latency,
                threshold_cost=threshold_cost,
                threshold_success_rate=threshold_success_rate,
                parallel_workers=parallel_workers,
                timeout_seconds=timeout_seconds
            )

            # Run evaluation
            results = evaluator.run_evaluation()

            # Print results
            print("\n" + "="*80)
            print("ğŸ“Š EVALUATION RESULTS")
            print("="*80)
            print(f"Overall Score: {results['overall_score']}/100")
            print(f"Status: {'âœ… PASSED' if results['passed'] else 'âŒ FAILED'}")
            print(f"\nMetrics:")
            print(f"  Accuracy:     {results['metrics']['accuracy']:.2%} (threshold: {threshold_accuracy:.2%})")
            print(f"  Avg Latency:  {results['metrics']['avg_latency_ms']:.2f}ms (threshold: {threshold_latency}ms)")
            print(f"  Avg Cost:     ${results['metrics']['avg_cost_per_request']:.4f} (threshold: ${threshold_cost})")
            print(f"  Success Rate: {results['metrics']['success_rate']:.2%} (threshold: {threshold_success_rate:.2%})")
            print(f"\nTest Summary:")
            print(f"  Total:   {results['metrics']['total_tests']}")
            print(f"  Passed:  {results['metrics']['passed_tests']}")
            print(f"  Failed:  {results['metrics']['failed_tests']}")
            print("="*80 + "\n")

            # Save report
            if generate_report:
                with open(report_path, 'w') as f:
                    json.dump(results, f, indent=2)
                print(f"ğŸ“„ Report saved to: {report_path}")

            # Set GitHub Actions outputs
            if os.getenv('GITHUB_OUTPUT'):
                with open(os.getenv('GITHUB_OUTPUT'), 'a') as f:
                    f.write(f"passed={str(results['passed']).lower()}\n")
                    f.write(f"overall_score={results['overall_score']}\n")
                    f.write(f"accuracy={results['metrics']['accuracy']:.4f}\n")
                    f.write(f"avg_latency={results['metrics']['avg_latency_ms']:.2f}\n")
                    f.write(f"avg_cost={results['metrics']['avg_cost_per_request']:.4f}\n")
                    f.write(f"success_rate={results['metrics']['success_rate']:.4f}\n")

            # Exit with appropriate code
            sys.exit(0 if results['passed'] else 1)


        if __name__ == '__main__':
            main()
        EVAL_SCRIPT_EOF

        chmod +x evaluate_agent.py

    - name: Run evaluation
      id: evaluate
      shell: bash
      env:
        GOLDEN_DATASET_PATH: ${{ inputs.golden-dataset-path }}
        THRESHOLD_ACCURACY: ${{ inputs.threshold-accuracy }}
        THRESHOLD_LATENCY: ${{ inputs.threshold-latency }}
        THRESHOLD_COST: ${{ inputs.threshold-cost }}
        THRESHOLD_SUCCESS_RATE: ${{ inputs.threshold-success-rate }}
        PARALLEL_WORKERS: ${{ inputs.parallel-workers }}
        TIMEOUT_SECONDS: ${{ inputs.timeout-seconds }}
        GENERATE_REPORT: ${{ inputs.generate-report }}
        REPORT_PATH: ${{ inputs.report-path }}
        API_ENDPOINT: ${{ inputs.api-endpoint }}
        API_KEY: ${{ inputs.api-key }}
      run: |
        echo "ğŸ¤– Running agent evaluation..."
        python3 evaluate_agent.py

    - name: Display results summary
      if: always()
      shell: bash
      run: |
        if [ -f "${{ inputs.report-path }}" ]; then
          echo "## Agent Evaluation Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          PASSED=$(jq -r '.passed' ${{ inputs.report-path }})
          SCORE=$(jq -r '.overall_score' ${{ inputs.report-path }})
          ACCURACY=$(jq -r '.metrics.accuracy' ${{ inputs.report-path }})
          LATENCY=$(jq -r '.metrics.avg_latency_ms' ${{ inputs.report-path }})
          COST=$(jq -r '.metrics.avg_cost_per_request' ${{ inputs.report-path }})

          if [ "$PASSED" = "true" ]; then
            echo "### âœ… Evaluation PASSED" >> $GITHUB_STEP_SUMMARY
          else
            echo "### âŒ Evaluation FAILED" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Overall Score:** $SCORE/100" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value | Threshold |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|-----------|" >> $GITHUB_STEP_SUMMARY
          echo "| Accuracy | $ACCURACY | ${{ inputs.threshold-accuracy }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Avg Latency | ${LATENCY}ms | ${{ inputs.threshold-latency }}ms |" >> $GITHUB_STEP_SUMMARY
          echo "| Avg Cost | \$${COST} | \$${{ inputs.threshold-cost }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
        fi

branding:
  icon: 'check-circle'
  color: 'green'
