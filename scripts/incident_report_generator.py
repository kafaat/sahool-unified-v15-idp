#!/usr/bin/env python3
"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
SAHOOL IDP - Automated Incident Report Generator
Ù…ÙˆÙ„Ø¯ ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø­ÙˆØ§Ø¯Ø« Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Generates comprehensive incident reports after system events:
- Database migrations
- Outages and recoveries
- Data integrity issues
- Performance incidents

Usage:
    python incident_report_generator.py --type migration --title "Orphaned Data Cleanup"

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import sys
import json
import argparse
import asyncio
import aiohttp
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Optional, Dict, Any, List
from dataclasses import dataclass, field, asdict
from enum import Enum
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("incident-report")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CONFIGURATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

REPORTS_DIR = Path("incident_reports")
REPORTS_DIR.mkdir(exist_ok=True)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ENUMS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class IncidentSeverity(Enum):
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    INFO = "info"

class IncidentStatus(Enum):
    OPEN = "open"
    INVESTIGATING = "investigating"
    IDENTIFIED = "identified"
    MONITORING = "monitoring"
    RESOLVED = "resolved"

class IncidentType(Enum):
    OUTAGE = "outage"
    DEGRADATION = "degradation"
    MIGRATION = "migration"
    DATA_INTEGRITY = "data_integrity"
    SECURITY = "security"
    PERFORMANCE = "performance"
    SCHEDULED = "scheduled_maintenance"

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DATA MODELS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

@dataclass
class TimelineEvent:
    timestamp: str
    description: str
    description_ar: str
    actor: str = "system"

@dataclass
class AffectedService:
    name: str
    impact: str
    recovery_time: Optional[str] = None

@dataclass
class ActionTaken:
    description: str
    description_ar: str
    outcome: str
    timestamp: str

@dataclass
class IncidentReport:
    # Identification
    id: str
    title: str
    title_ar: str
    incident_type: str
    severity: str
    status: str

    # Timing
    detected_at: str
    resolved_at: Optional[str] = None
    duration_minutes: Optional[int] = None

    # Description
    summary: str = ""
    summary_ar: str = ""
    root_cause: str = ""
    root_cause_ar: str = ""

    # Impact
    affected_services: List[Dict] = field(default_factory=list)
    affected_users_count: int = 0
    data_loss: bool = False
    data_loss_details: str = ""

    # Response
    timeline: List[Dict] = field(default_factory=list)
    actions_taken: List[Dict] = field(default_factory=list)

    # Prevention
    lessons_learned: List[str] = field(default_factory=list)
    preventive_measures: List[str] = field(default_factory=list)

    # Metadata
    created_by: str = "AI Technical Orchestrator"
    created_at: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())
    last_updated: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())

    def to_markdown(self) -> str:
        """Generate a Markdown incident report."""
        resolved_status = "âœ… Resolved" if self.status == "resolved" else f"ğŸ”„ {self.status.title()}"

        services_table = "\n".join([
            f"| {s.get('name', 'N/A')} | {s.get('impact', 'N/A')} | {s.get('recovery_time', 'N/A')} |"
            for s in self.affected_services
        ]) or "| No services affected | - | - |"

        timeline_list = "\n".join([
            f"- **{e.get('timestamp', '')}**: {e.get('description', '')} _{e.get('description_ar', '')}_"
            for e in self.timeline
        ]) or "- No timeline events recorded"

        actions_list = "\n".join([
            f"- {a.get('description', '')} â†’ _{a.get('outcome', '')}_"
            for a in self.actions_taken
        ]) or "- No actions recorded"

        lessons_list = "\n".join([f"- {l}" for l in self.lessons_learned]) or "- None identified"
        prevention_list = "\n".join([f"- {p}" for p in self.preventive_measures]) or "- None planned"

        return f'''# Incident Report: {self.title}
# ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø­Ø§Ø¯Ø«: {self.title_ar}

---

## Overview | Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©

| Field | Value |
|-------|-------|
| **Incident ID** | `{self.id}` |
| **Status** | {resolved_status} |
| **Severity** | {self.severity.upper()} |
| **Type** | {self.incident_type} |
| **Detected** | {self.detected_at} |
| **Resolved** | {self.resolved_at or "Ongoing"} |
| **Duration** | {self.duration_minutes or "N/A"} minutes |

---

## Summary | Ø§Ù„Ù…Ù„Ø®Øµ

**English:**
{self.summary}

**Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**
{self.summary_ar}

---

## Root Cause | Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ø¬Ø°Ø±ÙŠ

**English:**
{self.root_cause}

**Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©:**
{self.root_cause_ar}

---

## Impact | Ø§Ù„ØªØ£Ø«ÙŠØ±

| Affected Users | Data Loss |
|----------------|-----------|
| {self.affected_users_count} | {"Yes âš ï¸" if self.data_loss else "No âœ…"} |

{f"**Data Loss Details:** {self.data_loss_details}" if self.data_loss else ""}

### Affected Services | Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªØ£Ø«Ø±Ø©

| Service | Impact | Recovery Time |
|---------|--------|---------------|
{services_table}

---

## Timeline | Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø²Ù…Ù†ÙŠ

{timeline_list}

---

## Actions Taken | Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…ØªØ®Ø°Ø©

{actions_list}

---

## Lessons Learned | Ø§Ù„Ø¯Ø±ÙˆØ³ Ø§Ù„Ù…Ø³ØªÙØ§Ø¯Ø©

{lessons_list}

---

## Preventive Measures | Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ÙˆÙ‚Ø§Ø¦ÙŠØ©

{prevention_list}

---

## Metadata | Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©

- **Report Generated By:** {self.created_by}
- **Report Created:** {self.created_at}
- **Last Updated:** {self.last_updated}

---

*This report was automatically generated by SAHOOL IDP Incident Management System.*
*ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù‡Ø°Ø§ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨ÙˆØ§Ø³Ø·Ø© Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­ÙˆØ§Ø¯Ø« ÙÙŠ Ù…Ù†ØµØ© Ø³Ù‡ÙˆÙ„.*
'''

    def to_json(self) -> str:
        """Generate JSON representation."""
        return json.dumps(asdict(self), indent=2, ensure_ascii=False)

    def save(self):
        """Save report to files."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        base_name = f"incident_{self.id}_{timestamp}"

        # Save Markdown
        md_path = REPORTS_DIR / f"{base_name}.md"
        md_path.write_text(self.to_markdown(), encoding="utf-8")
        logger.info(f"ğŸ“ Saved Markdown report: {md_path}")

        # Save JSON
        json_path = REPORTS_DIR / f"{base_name}.json"
        json_path.write_text(self.to_json(), encoding="utf-8")
        logger.info(f"ğŸ“„ Saved JSON report: {json_path}")

        return md_path, json_path

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# REPORT TEMPLATES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class IncidentReportTemplates:
    """Pre-built templates for common incident types."""

    @staticmethod
    def orphaned_data_cleanup(
        orphaned_fields: int = 0,
        orphaned_sensors: int = 0,
        orphaned_tasks: int = 0,
        duration_minutes: int = 0,
        deadlock_occurred: bool = False,
    ) -> IncidentReport:
        """Template for orphaned data cleanup incidents."""
        now = datetime.now(timezone.utc)
        detected = now - timedelta(minutes=duration_minutes)

        timeline = [
            TimelineEvent(
                timestamp=detected.isoformat(),
                description="Orphaned data detected in database",
                description_ar="ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ø¨ÙŠØ§Ù†Ø§Øª ÙŠØªÙŠÙ…Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª",
                actor="DataGuardian-Agent"
            ),
            TimelineEvent(
                timestamp=(detected + timedelta(minutes=1)).isoformat(),
                description="Maintenance mode enabled",
                description_ar="ØªÙ… ØªÙØ¹ÙŠÙ„ ÙˆØ¶Ø¹ Ø§Ù„ØµÙŠØ§Ù†Ø©",
                actor="MaintenanceController"
            ),
        ]

        if deadlock_occurred:
            timeline.append(TimelineEvent(
                timestamp=(detected + timedelta(minutes=2)).isoformat(),
                description="Deadlock detected - automatic rollback initiated",
                description_ar="ØªÙ… Ø§ÙƒØªØ´Ø§Ù ØªØ¶Ø§Ø±Ø¨ - Ø¨Ø¯Ø¡ Ø§Ù„ØªØ±Ø§Ø¬Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ",
                actor="PostgreSQL"
            ))
            timeline.append(TimelineEvent(
                timestamp=(detected + timedelta(minutes=3)).isoformat(),
                description="Retry with maintenance mode - successful",
                description_ar="Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© ÙÙŠ ÙˆØ¶Ø¹ Ø§Ù„ØµÙŠØ§Ù†Ø© - Ù†Ø§Ø¬Ø­",
                actor="MigrationController"
            ))

        timeline.append(TimelineEvent(
            timestamp=now.isoformat(),
            description="Migration completed - maintenance mode disabled",
            description_ar="Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªÙ‡Ø¬ÙŠØ± - ØªÙ… ØªØ¹Ø·ÙŠÙ„ ÙˆØ¶Ø¹ Ø§Ù„ØµÙŠØ§Ù†Ø©",
            actor="MaintenanceController"
        ))

        total_cleaned = orphaned_fields + orphaned_sensors + orphaned_tasks

        return IncidentReport(
            id=f"INC-{now.strftime('%Y%m%d%H%M')}",
            title="Orphaned Data Cleanup Migration",
            title_ar="ØªÙ‡Ø¬ÙŠØ± ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙŠØªÙŠÙ…Ø©",
            incident_type=IncidentType.DATA_INTEGRITY.value,
            severity=IncidentSeverity.MEDIUM.value,
            status=IncidentStatus.RESOLVED.value,
            detected_at=detected.isoformat(),
            resolved_at=now.isoformat(),
            duration_minutes=duration_minutes,
            summary=f"Cleaned {total_cleaned} orphaned records ({orphaned_fields} fields, {orphaned_sensors} sensor readings, {orphaned_tasks} tasks) and added foreign key constraints to prevent future occurrences.",
            summary_ar=f"ØªÙ… ØªÙ†Ø¸ÙŠÙ {total_cleaned} Ø³Ø¬Ù„ ÙŠØªÙŠÙ… ({orphaned_fields} Ø­Ù‚Ù„ØŒ {orphaned_sensors} Ù‚Ø±Ø§Ø¡Ø© Ø­Ø³Ø§Ø³ØŒ {orphaned_tasks} Ù…Ù‡Ù…Ø©) ÙˆØ¥Ø¶Ø§ÙØ© Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø£Ø¬Ù†Ø¨ÙŠØ© Ù„Ù…Ù†Ø¹ Ø§Ù„ØªÙƒØ±Ø§Ø±.",
            root_cause="Missing foreign key constraints allowed creation of records referencing non-existent parent records (users, tenants, fields).",
            root_cause_ar="Ø³Ù…Ø­ ØºÙŠØ§Ø¨ Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø£Ø¬Ù†Ø¨ÙŠØ© Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø³Ø¬Ù„Ø§Øª ØªØ´ÙŠØ± Ø¥Ù„Ù‰ Ø³Ø¬Ù„Ø§Øª Ø£Ø¨ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© (Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†ØŒ Ù…Ø³ØªØ£Ø¬Ø±ÙŠÙ†ØŒ Ø­Ù‚ÙˆÙ„).",
            affected_services=[
                {"name": "field-service", "impact": "Fields not visible to users", "recovery_time": f"{duration_minutes} min"},
                {"name": "sensor-service", "impact": "Orphaned sensor data", "recovery_time": f"{duration_minutes} min"},
                {"name": "task-service", "impact": "Orphaned tasks", "recovery_time": f"{duration_minutes} min"},
            ],
            affected_users_count=orphaned_fields,  # Approximate
            data_loss=False,
            data_loss_details="All orphaned data was backed up before deletion. No valid user data was lost.",
            timeline=[asdict(e) for e in timeline],
            actions_taken=[
                {
                    "description": "Backed up orphaned records to recovery tables",
                    "description_ar": "Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„ÙŠØªÙŠÙ…Ø© ÙÙŠ Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø§Ø³ØªØ±Ø¯Ø§Ø¯",
                    "outcome": "Success",
                    "timestamp": (detected + timedelta(minutes=1)).isoformat(),
                },
                {
                    "description": f"Deleted {total_cleaned} orphaned records",
                    "description_ar": f"Ø­Ø°Ù {total_cleaned} Ø³Ø¬Ù„ ÙŠØªÙŠÙ…",
                    "outcome": "Success",
                    "timestamp": (now - timedelta(minutes=1)).isoformat(),
                },
                {
                    "description": "Added ON DELETE CASCADE foreign key constraints",
                    "description_ar": "Ø¥Ø¶Ø§ÙØ© Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø£Ø¬Ù†Ø¨ÙŠØ© Ù…Ø¹ Ø§Ù„Ø­Ø°Ù Ø§Ù„Ù…ØªØªØ§Ù„ÙŠ",
                    "outcome": "Success",
                    "timestamp": now.isoformat(),
                },
            ],
            lessons_learned=[
                "Foreign key constraints should be added during initial schema design",
                "ÙŠØ¬Ø¨ Ø¥Ø¶Ø§ÙØ© Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø£Ø¬Ù†Ø¨ÙŠØ© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØµÙ…ÙŠÙ… Ø§Ù„Ø£ÙˆÙ„ÙŠ Ù„Ù„Ù…Ø®Ø·Ø·",
                "Orphaned data accumulates silently and causes user-facing issues",
                "Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙŠØªÙŠÙ…Ø© ØªØªØ±Ø§ÙƒÙ… Ø¨ØµÙ…Øª ÙˆØªØ³Ø¨Ø¨ Ù…Ø´Ø§ÙƒÙ„ Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†",
            ],
            preventive_measures=[
                "Add foreign key constraint checks to CI/CD pipeline",
                "Ø¥Ø¶Ø§ÙØ© ÙØ­ÙˆØµØ§Øª Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø£Ø¬Ù†Ø¨ÙŠØ© Ø¥Ù„Ù‰ Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ CI/CD",
                "Implement weekly orphaned data detection job",
                "ØªÙ†ÙÙŠØ° Ù…Ù‡Ù…Ø© Ø£Ø³Ø¨ÙˆØ¹ÙŠØ© Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙŠØªÙŠÙ…Ø©",
                "Add database schema linting to pre-commit hooks",
                "Ø¥Ø¶Ø§ÙØ© ÙØ­Øµ Ù…Ø®Ø·Ø· Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ pre-commit hooks",
            ],
        )

    @staticmethod
    def database_migration_failure(
        migration_name: str,
        error_message: str,
        rollback_successful: bool,
        duration_minutes: int = 0,
    ) -> IncidentReport:
        """Template for failed database migration incidents."""
        now = datetime.now(timezone.utc)
        detected = now - timedelta(minutes=duration_minutes)

        return IncidentReport(
            id=f"INC-{now.strftime('%Y%m%d%H%M')}",
            title=f"Database Migration Failure: {migration_name}",
            title_ar=f"ÙØ´Ù„ ØªÙ‡Ø¬ÙŠØ± Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {migration_name}",
            incident_type=IncidentType.MIGRATION.value,
            severity=IncidentSeverity.HIGH.value,
            status=IncidentStatus.RESOLVED.value if rollback_successful else IncidentStatus.INVESTIGATING.value,
            detected_at=detected.isoformat(),
            resolved_at=now.isoformat() if rollback_successful else None,
            duration_minutes=duration_minutes,
            summary=f"Migration '{migration_name}' failed with error: {error_message}. {'Rollback was successful.' if rollback_successful else 'Manual intervention required.'}",
            summary_ar=f"ÙØ´Ù„ Ø§Ù„ØªÙ‡Ø¬ÙŠØ± '{migration_name}' Ù…Ø¹ Ø§Ù„Ø®Ø·Ø£: {error_message}. {'ÙƒØ§Ù† Ø§Ù„ØªØ±Ø§Ø¬Ø¹ Ù†Ø§Ø¬Ø­Ø§Ù‹.' if rollback_successful else 'Ù…Ø·Ù„ÙˆØ¨ ØªØ¯Ø®Ù„ ÙŠØ¯ÙˆÙŠ.'}",
            root_cause=error_message,
            root_cause_ar=error_message,
            affected_services=[
                {"name": "database", "impact": "Schema change failed", "recovery_time": f"{duration_minutes} min" if rollback_successful else "Unknown"},
            ],
            data_loss=not rollback_successful,
            timeline=[
                {"timestamp": detected.isoformat(), "description": f"Migration {migration_name} started", "description_ar": f"Ø¨Ø¯Ø¡ Ø§Ù„ØªÙ‡Ø¬ÙŠØ± {migration_name}", "actor": "MigrationController"},
                {"timestamp": (detected + timedelta(minutes=1)).isoformat(), "description": f"Error occurred: {error_message[:50]}...", "description_ar": f"Ø­Ø¯Ø« Ø®Ø·Ø£", "actor": "PostgreSQL"},
                {"timestamp": now.isoformat(), "description": "Rollback executed" if rollback_successful else "Manual intervention required", "description_ar": "ØªÙ… ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ±Ø§Ø¬Ø¹" if rollback_successful else "Ù…Ø·Ù„ÙˆØ¨ ØªØ¯Ø®Ù„ ÙŠØ¯ÙˆÙŠ", "actor": "MigrationController"},
            ],
            actions_taken=[
                {
                    "description": "Automatic rollback initiated",
                    "description_ar": "Ø¨Ø¯Ø¡ Ø§Ù„ØªØ±Ø§Ø¬Ø¹ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ",
                    "outcome": "Success" if rollback_successful else "Failed",
                    "timestamp": now.isoformat(),
                },
            ],
            lessons_learned=[
                "Always test migrations on a replica first",
                "Run migrations during low-traffic periods",
            ],
            preventive_measures=[
                "Add migration dry-run to CI/CD pipeline",
                "Implement shadow database for testing",
            ],
        )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# CLI INTERFACE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    parser = argparse.ArgumentParser(
        description="SAHOOL Incident Report Generator - Ù…ÙˆÙ„Ø¯ ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø­ÙˆØ§Ø¯Ø«"
    )

    parser.add_argument("--type", "-t", choices=["migration", "outage", "data", "custom"],
                       default="custom", help="Incident type")
    parser.add_argument("--title", required=True, help="Incident title")
    parser.add_argument("--title-ar", help="Incident title (Arabic)")
    parser.add_argument("--severity", choices=["critical", "high", "medium", "low", "info"],
                       default="medium", help="Severity level")
    parser.add_argument("--duration", type=int, default=0, help="Duration in minutes")

    # For orphaned data template
    parser.add_argument("--orphaned-fields", type=int, default=0)
    parser.add_argument("--orphaned-sensors", type=int, default=0)
    parser.add_argument("--orphaned-tasks", type=int, default=0)
    parser.add_argument("--deadlock", action="store_true")

    args = parser.parse_args()

    if args.type == "data":
        report = IncidentReportTemplates.orphaned_data_cleanup(
            orphaned_fields=args.orphaned_fields,
            orphaned_sensors=args.orphaned_sensors,
            orphaned_tasks=args.orphaned_tasks,
            duration_minutes=args.duration,
            deadlock_occurred=args.deadlock,
        )
    else:
        # Custom report
        report = IncidentReport(
            id=f"INC-{datetime.now().strftime('%Y%m%d%H%M')}",
            title=args.title,
            title_ar=args.title_ar or args.title,
            incident_type=args.type,
            severity=args.severity,
            status="resolved",
            detected_at=datetime.now(timezone.utc).isoformat(),
            duration_minutes=args.duration,
        )

    # Save the report
    md_path, json_path = report.save()

    # Print summary
    print("\n" + "=" * 70)
    print("  ğŸ“‹ INCIDENT REPORT GENERATED")
    print("=" * 70)
    print(f"  ID:       {report.id}")
    print(f"  Title:    {report.title}")
    print(f"  Severity: {report.severity.upper()}")
    print(f"  Status:   {report.status}")
    print(f"  Files:")
    print(f"    - {md_path}")
    print(f"    - {json_path}")
    print("=" * 70 + "\n")

if __name__ == "__main__":
    main()
